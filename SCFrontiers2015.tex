\documentclass{superfri}

\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[cmex10]{amsmath}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{fancyvrb}
\usepackage{ifthen}
\usepackage{cite}
\usepackage{tabulary}
\usepackage{url}
\usepackage{xspace}
\usepackage[pdfborder={0 0 0}]{hyperref}
\usepackage{verbatim}

\usepackage{color}
\definecolor{yellow}{rgb}{1,1,0}
\definecolor{black}{rgb}{0,0,0}
\definecolor{ltcyan}{rgb}{.75,1,1}
\definecolor{red}{rgb}{1,0,0}
\definecolor{gray}{rgb}{.6,.6,.6}
\definecolor{darkred}{rgb}{0.5,0,0}
\definecolor{darkgreen}{rgb}{0,0.5,0}

% Cite commands I use to abstract away the different ways to reference an
% entry in the bibliography (superscripts, numbers, dates, or author
% abbreviations).  \scite is a short cite that is used immediately after
% when the authors are mentioned.  \lcite is a full citation that is used
% anywhere.  Both should be used right next to the text being cited without
% any spacing.
\newcommand*{\lcite}[1]{~\cite{#1}}
\newcommand*{\scite}[1]{~\cite{#1}}

\newcommand{\etal}{et al.\xspace}

\newcommand*{\keyterm}[1]{\emph{#1}}

\newcommand{\fix}[1]{{\color{red}\textsc{[#1]}}}
%\newcommand{\fix}[1]{}

% Avoid putting figures on their own page.
\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.95}
\renewcommand{\bottomfraction}{0.95}

% Make sure this is big enough so that only big figures end up on their own
% page but small enough so that if a figure does have to be on its own
% page, it won't push everything to the bottom because it's not big enough
% to have its own page.
\renewcommand{\floatpagefraction}{.75}

\begin{document}

%\author{I.M.~Scientist\footnote{\label{susu}South Ural State University}, U.R.~Author\footnoteref{susu}}
\author{
  Kenneth Moreland\footnote{Sandia National Laboratories},
  Hank Childs\footnote{University of Oregon},
  \fix{others?}
  }

\title{\fix{Hank's talk title.}}

\maketitle{}

\begin{abstract}%
  \fix{Start with Hank's abstract, which I do not have right now.}

  \keywords{scientific visualization, exascale}
\end{abstract}


\section*{Introduction}
\label{sec:Introduction}

\noindent
Although the basic architecture for high-performance computing platforms has
remained homogeneous and consistent for over a decade, revolutionary changes
are coming. Power constraints and physical limitations are impelling the
use of new types of processors, heterogeneous architectures, and deeper
memory and storage hierarchies. Such drastic changes propagate to the
design of software that is run on these high-performance computers and how
we use them.

The predictions for extreme-scale computing are dire.  Recent trends, many
of which are driven by power budgets, which max out at
20~MW\lcite{ExascaleArchitecturesReport}, indicate that future
high-performance computers will have different hardware structure and
programming models to which software must adapt. The predicted changes from
petascale to exascale are summarized in
Table~\ref{table:PetascaleVsExascale} where the performance factor change
differs up to five orders of magnitude for different components of the
system.

\begin{table}[htdp]
  \centering
  \caption{Comparison of a petascale supercomputer to an expected exascale
    supercomputer\lcite{ScientificDiscoveryExascale2011}.}
  \label{table:PetascaleVsExascale}
  \begin{tabular}{@{}lcccc@{}}
    \toprule
    & & \multicolumn{2}{c}{Exascale (Prediction)} & \\
    System Parameter & Petascale & Swim Lane 1 & Swim Lane 2 & Factor Change \\
    \midrule
    System Peak & 2~PF & \multicolumn{2}{c}{1~EF} & 500 \\
    Power & 6~MW & \multicolumn{2}{c}{$\le$20~MW} & 3\\
    System Memory & 0.3~PB & \multicolumn{2}{c}{32--64~PB} & 100--200 \\ % San Diego report says 50PB
    Total Concurrency & 225~K & 1~B$\times$10 & 1~B$\times$100 & 40,000--400,000 \\
    Node Performance & 125~GF & 1~TF & 10~TF & 8--80\\
    %Node Memory BW & 25~GB/s & 400~GB/s & 4~TB/s & 16 \\
    Node Core Count & 12 & 1,000 & 10,000 & 83--830 \\
    Network BW & 1.5~GB/s & 100~GB/s & 1,000~GB/s & 66--660 \\
    System Size (nodes) & 18700 & 1,000,000 & 100,000 & 50--500 \\
    I/O Capacity & 15~PB & \multicolumn{2}{c}{300--1,000~PB} & 20--67\\
    I/O BW & 0.2~TB/s & \multicolumn{2}{c}{20--60~TB/s} & 100--300 \\
    \bottomrule
  \end{tabular}
\end{table}

A particularly alarming feature of Table~\ref{table:PetascaleVsExascale} is
the increase in concurrency of the system: up to 5 orders of magnitude. We
currently stand about halfway through the transition from petascale to
exascale and we can observe this prediction coming to fruition through the
use of accelerator or many-core processors. In the November 2014 Top500
supercomputer list, 75 of the computers contain many-core components
including half of the top 10 computers.

A many-core processor achieves high instruction bandwidth by packing many
cores onto a single process. To achieve the highest density of cores at the
lowest possible power requirement, these cores are trimmed of
latency-hiding features and require careful coordination to achieve peak
performance. Although very scalable on distributed memory architectures,
our current parallel scientific visualization tools such as
VisIt\lcite{VisIt} and ParaView\lcite{ParaView} are inadequate on these
machines.

Overhauling our software tools is one of the principal visualization
research challenges today\lcite{Childs2013}. This paper reviews scientific
visualization using data parallel primitives. This approach enables
simplified algorithm development and helps achieve portable performance.


\section{Data Parallel Primitives}

\fix{what they are, how they help}

\fix{discuss Blelloch briefly}

\fix{DAX/EAVL/PISTON, leading into VTK-m}


\section{Patterns for Data Parallel Visualization}

\fix{data parallel primitives in action (focusing on ``re-thinking algs'')}

\fix{show an example of isosurface}


\section{Results}

\fix{Show ray-tracing results}


\section{Conclusion}

\fix{conclude DPP is viable}


\ack{
  This material is based upon work supported by the U.S. Department of
  Energy, Office of Science, Office of Advanced Scientific Computing
  Research, under Award Numbers 10-014707, 12-015215, and 14-017566.

  Sandia National Laboratories is a multi-program laboratory managed and
  operated by Sandia Corporation, a wholly owned subsidiary of Lockheed
  Martin Corporation, for the U.S. Department of Energy's National Nuclear
  Security Administration under contract DE-AC04-94AL85000.
}


\bibliographystyle{plain}
\bibliography{SCFrontiers2015}

\end{document}
